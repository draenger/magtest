{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test session id: 20241110104819\n"
     ]
    }
   ],
   "source": [
    "# Generate test session id based on current time year, month, day, hour, minute, second yyyymmddhhmmss\n",
    "import datetime\n",
    "test_session_id = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "print(f\"Test session id: {test_session_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Models:\n",
      "==============\n",
      "- gpt-4o\n",
      "- gpt-4o-mini\n",
      "- gpt-4-turbo\n",
      "- gpt-4\n",
      "- gpt-3.5-turbo-0125\n",
      "- gemini-1.5-flash-002\n",
      "- gemini-1.5-flash-001\n",
      "- gemini-1.5-pro-002\n",
      "- gemini-1.5-pro-001\n",
      "- gemini-1.0-pro-002\n",
      "- gemini-1.0-pro-001\n",
      "\n",
      "Total loaded models: 11\n",
      "\n",
      "No model list filter applied.\n",
      "Loaded Benchmarks:\n",
      "=================\n",
      "- mmlu-0shot\n",
      "- mmlu-5shot\n",
      "- gsm8k-0shot\n",
      "- gsm8k-4shot\n",
      "- bbh-0shot\n",
      "- bbh-3shot\n",
      "\n",
      "Total loaded benchmarks: 6\n",
      "\n",
      "No benchmark list filter applied.\n"
     ]
    }
   ],
   "source": [
    "from data import Database\n",
    "from data.repositories import (\n",
    "    PreparedQuestionRepository,\n",
    "    ModelResultRepository,\n",
    "    MMLUQuestionRepository,\n",
    "    BatchJobRepository,\n",
    "    GSM8KQuestionRepository,\n",
    "    BBHQuestionRepository\n",
    ")\n",
    "from benchmarks import BenchmarkRunner, BenchmarkRegistry\n",
    "from benchmarks.benchmark_summary import BenchmarkSummary\n",
    "from ai_models import ModelRegistry, BatchProgressManager\n",
    "\n",
    "# Initialize database\n",
    "db = Database()\n",
    "db.create_all_tables()\n",
    "mmlu_question_repository = MMLUQuestionRepository(db)\n",
    "gsm8k_question_repository = GSM8KQuestionRepository(db)\n",
    "bbh_question_repository = BBHQuestionRepository(db)\n",
    "prepared_question_repo = PreparedQuestionRepository(db)\n",
    "model_result_repo = ModelResultRepository(db)\n",
    "batch_job_repo = BatchJobRepository(db)\n",
    "benchmark_summary = BenchmarkSummary(db)\n",
    "batch_manager = BatchProgressManager(batch_job_repo)\n",
    "\n",
    "test_session_id = 20241108214342\n",
    "model_name_list = [] # all models\n",
    "# model_name_list = [\"gpt-4o-mini\"] # only gpt-4o-mini\n",
    "# model_name_list = [\"gpt-4o-mini\", \"claude-3-haiku-20240307\"]\n",
    "# model_name_list = [\"gemini-1.5-flash-002\"]  # only gemini-1.5-flash\n",
    "# model_name_list = [\"claude-3-haiku-20240307\"]\n",
    "# model_name_list = [\"gemini-1.5-flash-002\"]\n",
    "# model_name_list = [\"gemini-1.5-flash-002\", \"gpt-4o-mini\", \"claude-3-haiku-20240307\"]\n",
    "\n",
    "# Register models\n",
    "model_registry = ModelRegistry(model_name_list)\n",
    "model_registry.register_production_models()\n",
    "model_registry.print_loaded_models()\n",
    "\n",
    "\n",
    "# benchmark_name_list = [\"MMLU-0Shot\", \"MMLU-5Shot\"]\n",
    "# benchmark_name_list = [\"GSM8K-0Shot\"]\n",
    "# benchmark_name_list = [\"BBH-3Shot\"]\n",
    "benchmark_name_list = [] # all benchmarks\n",
    "\n",
    "# Register benchmarks\n",
    "benchmark_registry = BenchmarkRegistry(\n",
    "    mmlu_question_repository,\n",
    "    gsm8k_question_repository,\n",
    "    bbh_question_repository,\n",
    "    prepared_question_repo, \n",
    "    model_result_repo, \n",
    "    batch_job_repo,\n",
    "    test_session_id = test_session_id, \n",
    "    benchmark_name_list = benchmark_name_list)\n",
    "\n",
    "benchmark_registry.register_benchmarks()\n",
    "benchmark_registry.print_loaded_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run the benchmark runner\n",
    "runner = BenchmarkRunner(model_registry, benchmark_registry)\n",
    "# runner.estimate_model_results()\n",
    "\n",
    "# benchmark_summary.print_full_summary(test_session_id)\n",
    "# benchmark_summary.print_benchmark_summary(test_session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runner.run_benchmarks(in_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_manager.show_batch_progress_from_db(test_session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runner.check_and_process_batch_results()\n",
    "# benchmark_summary.print_full_summary(test_session_id)\n",
    "#print(\"\\n\")\n",
    "#benchmark_summary.print_benchmark_summary(test_session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BenchmarkSummary' object has no attribute '_get_model_performance_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# benchmark_summary.print_full_summary(test_session_id)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# benchmark_summary.save_full_summary_to_excel(test_session_id, \"full_summary.xlsx\")\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# benchmark_summary.create_all_plots(test_session_id)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# benchmark_summary.plot_top_cost_effective_models(test_session_id)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# benchmark_summary.analyze_openai_models_comparison(test_session_id)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mbenchmark_summary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_openai_vs_google_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_session_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Master\\Repo\\Magtest\\src\\benchmarks\\benchmark_summary.py:1048\u001b[0m, in \u001b[0;36mBenchmarkSummary.plot_openai_vs_google_comparison\u001b[1;34m(self, test_session_id)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;124;03mCreates a plot comparing cost effectiveness between OpenAI and Google models.\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# Get data for all models\u001b[39;00m\n\u001b[1;32m-> 1048\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model_performance_data\u001b[49m(test_session_id)\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;66;03m# Filter and categorize models\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m openai_models \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1052\u001b[0m     k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1053\u001b[0m }\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BenchmarkSummary' object has no attribute '_get_model_performance_data'"
     ]
    }
   ],
   "source": [
    "# benchmark_summary.print_full_summary(test_session_id)\n",
    "# benchmark_summary.save_full_summary_to_excel(test_session_id, \"full_summary.xlsx\")\n",
    "# benchmark_summary.create_all_plots(test_session_id)\n",
    "# benchmark_summary.plot_cost_analysis(test_session_id)\n",
    "# benchmark_summary.plot_cost_effectiveness(test_session_id)\n",
    "# benchmark_summary.analyze_few_shot_impact(test_session_id)\n",
    "# benchmark_summary.plot_top_cost_effective_models(test_session_id)\n",
    "# benchmark_summary.analyze_openai_models_comparison(test_session_id)\n",
    "benchmark_summary.plot_openai_vs_google_comparison(test_session_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
