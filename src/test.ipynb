{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping model claude-3-5-sonnet-20240620 as it's not in the model_name_list\n",
      "Skipping model claude-3-opus-20240229 as it's not in the model_name_list\n",
      "Skipping model claude-3-sonnet-20240229 as it's not in the model_name_list\n",
      "Skipping model claude-3-haiku-20240307 as it's not in the model_name_list\n",
      "Skipping model gpt-4o as it's not in the model_name_list\n",
      "Skipping model gpt-4-turbo as it's not in the model_name_list\n",
      "Skipping model gpt-4 as it's not in the model_name_list\n",
      "Skipping model gpt-3.5-turbo-0125 as it's not in the model_name_list\n",
      "Loaded Models:\n",
      "==============\n",
      "- gpt-4o-mini\n",
      "\n",
      "Total loaded models: 1\n",
      "\n",
      "Filtered by model list: gpt-4o-mini\n",
      "Data already exists for benchmark MMUL-0Shot and session 17112010001\n",
      "Estimation already exists for benchmark MMUL-0Shot, session 17112010001, and model gpt-4o-mini\n",
      "Data already exists for benchmark MMUL-5Shot and session 17112010001\n",
      "Estimation already exists for benchmark MMUL-5Shot, session 17112010001, and model gpt-4o-mini\n",
      "Benchmark Summary\n",
      "=================\n",
      "     Benchmark              Model                Queries             Avg Score       Total Execution Time      Est Tokens           Act Tokens            Est Cost             Act Cost      \n",
      "     MMUL-0Shot          gpt-4o-mini              570                 0.7754                0.0000                61935                65688               0.0095               0.0051       \n",
      "     MMUL-5Shot          gpt-4o-mini              570                 0.7526                0.0000               350420               353927               0.0528               0.0267       \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data import Database\n",
    "from data.repositories import (\n",
    "    PreparedQuestionRepository,\n",
    "    ModelResultRepository,\n",
    "    MMULQuestionRepository,\n",
    "    BatchJobRepository\n",
    ")\n",
    "from benchmarks import BenchmarkRunner, BenchmarkRegistry\n",
    "from benchmarks.benchmark_summary import BenchmarkSummary\n",
    "from ai_models import ModelRegistry\n",
    "\n",
    "# Initialize database\n",
    "db = Database()\n",
    "db.create_all_tables()\n",
    "mmul_question_repository = MMULQuestionRepository(db)\n",
    "prepared_question_repo = PreparedQuestionRepository(db)\n",
    "model_result_repo = ModelResultRepository(db)\n",
    "batch_job_repo = BatchJobRepository(db)\n",
    "benchmark_summary = BenchmarkSummary(db)\n",
    "\n",
    "test_session_id = 17112010002\n",
    "\n",
    "# model_name_list = [] # all models\n",
    "model_name_list = [\"gpt-4o-mini\"] # only gpt-4o-mini\n",
    "\n",
    "\n",
    "# Register models\n",
    "model_registry = ModelRegistry(model_name_list)\n",
    "model_registry.register_production_models()\n",
    "model_registry.print_loaded_models()\n",
    "\n",
    "# Register benchmarks\n",
    "benchmark_registry = BenchmarkRegistry(\n",
    "    mmul_question_repository,\n",
    "    prepared_question_repo, \n",
    "    model_result_repo, \n",
    "    batch_job_repo,\n",
    "    test_session_id = test_session_id, \n",
    "    max_tests_per_benchmark = 10)\n",
    "\n",
    "benchmark_registry.register_mmul_benchmarks()\n",
    "\n",
    "# Create and run the benchmark runner\n",
    "runner = BenchmarkRunner(model_registry, benchmark_registry)\n",
    "runner.estimate_model_results()\n",
    "\n",
    "# benchmark_summary.print_full_summary(test_session_id)\n",
    "benchmark_summary.print_benchmark_summary(test_session_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Data already exists for benchmark MMUL-0Shot and session 17112010001\n",
      "Batch job already exists for Benchmark[MMUL-0Shot], Session[17112010001], model[gpt-4o-mini]\n",
      "Data already exists for benchmark MMUL-5Shot and session 17112010001\n",
      "Batch job already exists for Benchmark[MMUL-5Shot], Session[17112010001], model[gpt-4o-mini]\n",
      "\n",
      "\n",
      "Data already exists for benchmark MMUL-0Shot and session 17112010001\n",
      "Batch status: completed\n",
      "Data already exists for benchmark MMUL-5Shot and session 17112010001\n",
      "Batch status: completed\n",
      "\n",
      "\n",
      "Benchmark Summary\n",
      "=================\n",
      "     Benchmark              Model                Queries             Avg Score       Total Execution Time      Est Tokens           Act Tokens            Est Cost             Act Cost      \n",
      "     MMUL-0Shot          gpt-4o-mini              570                 0.7754                0.0000                61935                65688               0.0095               0.0051       \n",
      "     MMUL-5Shot          gpt-4o-mini              570                 0.7526                0.0000               350420               353927               0.0528               0.0267       \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "runner.run_benchmarks(in_batch=True)\n",
    "print(\"\\n\")\n",
    "runner.check_and_process_batch_results()\n",
    "# benchmark_summary.print_full_summary(test_session_id)\n",
    "print(\"\\n\")\n",
    "benchmark_summary.print_benchmark_summary(test_session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: batch_67152006844c8190ac2d4f9a677d7fd1\n",
      "Status: completed\n",
      "Total requests: 570\n",
      "Progress: [                                                  ]\n",
      "Completed: 570 | Failed: 0 | In Progress: 0\n",
      "Completed: 100.00% | Failed: 0.00% | In Progress: 0.00%\n",
      "------------------------------------------------------------\n",
      "Batch ID: batch_671520098fdc8190828ed4e7f2aa5465\n",
      "Status: completed\n",
      "Total requests: 570\n",
      "Progress: [                                                  ]\n",
      "Completed: 570 | Failed: 0 | In Progress: 0\n",
      "Completed: 100.00% | Failed: 0.00% | In Progress: 0.00%\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from colorama import Fore, Back, Style, init\n",
    "\n",
    "# Inicjalizacja colorama\n",
    "init()\n",
    "\n",
    "# Załaduj zmienne środowiskowe z pliku .env\n",
    "load_dotenv()\n",
    "\n",
    "# Inicjalizacja klienta OpenAI\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def get_batch_info(batch_id):\n",
    "    try:\n",
    "        return client.batches.retrieve(batch_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Wystąpił błąd podczas pobierania informacji o batchu {batch_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def display_batch_progress(batch_info):\n",
    "    if not batch_info:\n",
    "        return\n",
    "\n",
    "    total = batch_info.request_counts.total\n",
    "    completed = batch_info.request_counts.completed\n",
    "    failed = batch_info.request_counts.failed\n",
    "    in_progress = total - completed - failed\n",
    "\n",
    "    print(f\"Batch ID: {batch_info.id}\")\n",
    "    print(f\"Status: {batch_info.status}\")\n",
    "    print(f\"Total requests: {total}\")\n",
    "\n",
    "    bar_length = 50\n",
    "    completed_length = int(completed / total * bar_length)\n",
    "    failed_length = int(failed / total * bar_length)\n",
    "    in_progress_length = bar_length - completed_length - failed_length\n",
    "\n",
    "    progress_bar = (\n",
    "        Fore.GREEN + Back.GREEN + \" \" * completed_length +\n",
    "        Fore.RED + Back.RED + \" \" * failed_length +\n",
    "        Fore.YELLOW + Back.YELLOW + \" \" * in_progress_length +\n",
    "        Style.RESET_ALL\n",
    "    )\n",
    "\n",
    "    print(f\"Progress: [{progress_bar}]\")\n",
    "    print(f\"Completed: {completed} | Failed: {failed} | In Progress: {in_progress}\")\n",
    "    \n",
    "    # Dodajemy procentowe wartości dla lepszej czytelności\n",
    "    completed_percent = (completed / total) * 100\n",
    "    failed_percent = (failed / total) * 100\n",
    "    in_progress_percent = (in_progress / total) * 100\n",
    "    \n",
    "    print(f\"Completed: {completed_percent:.2f}% | Failed: {failed_percent:.2f}% | In Progress: {in_progress_percent:.2f}%\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Lista ID batchów\n",
    "batch_ids = [\n",
    "    \"batch_67152006844c8190ac2d4f9a677d7fd1\",\n",
    "    \"batch_671520098fdc8190828ed4e7f2aa5465\"\n",
    "]\n",
    "\n",
    "for batch_id in batch_ids:\n",
    "    batch_info = get_batch_info(batch_id)\n",
    "    if batch_info:\n",
    "        display_batch_progress(batch_info)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
